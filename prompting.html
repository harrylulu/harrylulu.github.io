<br> in-context learning = zero shot inference: include input data within the prompt. Large LLM is good at zero-shot but not trained on specific task </br> 
<br> one shot inference: include the example that demonstrates the tasks to be carried out to the model </br>
<br> few shot inference: include multiple examples </br>
<br> context window: limit on the amount of in-context learning that you can pass in to the model </br>
<br> if few shot doesn't work well, should try fine-tuning which trains on additional data to make it more capable of the task you want it to perform </br>
